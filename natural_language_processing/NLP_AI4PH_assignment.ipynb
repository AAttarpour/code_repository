{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Ahmadreza Attarpour  \n",
    "**Email:** [a.attarpour@mail.utoronto.ca](mailto:a.attarpour@mail.utoronto.ca)  \n",
    "\n",
    "This is my answers notebook #1 to the assignment of NLP course at AI4PH\n",
    "\n",
    "-Load the Brown Corpus from NLTK using paras(). \n",
    "\n",
    "-Remove punctuation and stopwords. \n",
    "\n",
    "-Apply the lancaster stemmer. \n",
    "\n",
    "-Print to the screen the top 10 words in terms of TF. Show the TF values as well. \n",
    "\n",
    "-Print to the screen the top 10 words in terms of TF-IDF. Use the paragraphs as documents for calculating TF-IDF. Show the TF-IDF values as well. \n",
    "\n",
    "-Use pos_tag() to tag each token. \n",
    "\n",
    "-Print to the screen the 10 most common trigrams of word-tag pairs. Show their frequencies as well. Use nltk.trigrams(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/ahmadreza/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all packages used in this file\n",
    "from nltk.corpus import brown\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import operator\n",
    "from collections import Counter\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Brown Corpus from NLTK using paras(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Load the Brown Corpus using paras()\n",
    "brown_paras = brown.paras()\n",
    "# Print the first paragraph\n",
    "print(brown_paras[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove punctuation and stopwords. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove punctuation and stopwords\n",
    "def clean_paragraph(paragraph):\n",
    "    cleaned_paragraph = []\n",
    "    for sentence in paragraph:\n",
    "        cleaned_sentence = [word.lower() for word in sentence if word.lower() not in set(stopwords.words('english')) and word not in string.punctuation and re.match(r'^\\w+$', word)]\n",
    "        cleaned_paragraph.append(cleaned_sentence)\n",
    "    return cleaned_paragraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fulton', 'county', 'grand', 'jury', 'said', 'friday', 'investigation', 'recent', 'primary', 'election', 'produced', 'evidence', 'irregularities', 'took', 'place']]\n"
     ]
    }
   ],
   "source": [
    "# Clean the paragraphs\n",
    "cleaned_brown_paras = [clean_paragraph(paragraph) for paragraph in brown_paras]\n",
    "\n",
    "# Print the first cleaned paragraph\n",
    "print(cleaned_brown_paras[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply the lancaster stemmer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lancaster = nltk.LancasterStemmer()\n",
    "# Function to stem the words in the paragraphs\n",
    "def lancaster_stemmer(paragraph):\n",
    "    lancaster_stems = []\n",
    "    for sentence in paragraph:\n",
    "        sentence_stem = [lancaster.stem(word) for word in sentence]\n",
    "        lancaster_stems.append(sentence_stem)\n",
    "    return lancaster_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['fulton', 'county', 'grand', 'jury', 'said', 'friday', 'investig', 'rec', 'prim', 'elect', 'produc', 'evid', 'irregul', 'took', 'plac']]\n"
     ]
    }
   ],
   "source": [
    "# stem the words in the paragraphs\n",
    "cleaned_brown_paras_stems = [lancaster_stemmer(paragraph) for paragraph in cleaned_brown_paras]\n",
    "\n",
    "# Print the first cleaned paragraph\n",
    "print(cleaned_brown_paras_stems[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print to the screen the top 10 words in terms of TF. Show the TF values as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fulton', 'county', 'grand', 'jury', 'said', 'friday', 'investig', 'rec', 'prim', 'elect']\n"
     ]
    }
   ],
   "source": [
    "tokens_clean = [word for paragraph in cleaned_brown_paras_stems for sentence in paragraph for word in sentence]\n",
    "\n",
    "print(tokens_clean[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on: 3431\n",
      "would: 2715\n",
      "us: 2490\n",
      "stat: 2095\n",
      "said: 1961\n",
      "tim: 1957\n",
      "ev: 1944\n",
      "new: 1785\n",
      "man: 1700\n",
      "year: 1620\n"
     ]
    }
   ],
   "source": [
    "# Calculate term frequency\n",
    "tf_clean = nltk.FreqDist(tokens_clean)\n",
    "\n",
    "# Print the top 10 words along with their TF values\n",
    "top_10_words = tf_clean.most_common(10)\n",
    "for word, freq in top_10_words:\n",
    "    print(f'{word}: {freq}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print to the screen the top 10 words in terms of TF-IDF. Use the paragraphs as documents for calculating TF-IDF. Show the TF-IDF values as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on: 6096.876533436755\n",
      "would: 5713.5908380628825\n",
      "us: 5275.538945943473\n",
      "stat: 5012.2550712287175\n",
      "tim: 4423.8485854492155\n",
      "ev: 4364.9440280267145\n",
      "new: 4321.154824085933\n",
      "said: 4259.562959566486\n",
      "man: 4164.951740043983\n",
      "af: 4083.3627652471705\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate TF-IDF\n",
    "tf_idf = {}\n",
    "doc_size = len(brown_paras)\n",
    "ndocs = len(cleaned_brown_paras_stems)\n",
    "\n",
    "for token in tf_clean:\n",
    "    count = 0\n",
    "    for paragraph in cleaned_brown_paras_stems:\n",
    "        if any(token in sentence for sentence in paragraph):\n",
    "            count += 1\n",
    "    tf_idf[token] = tf_clean[token] * math.log(ndocs / (1 + count))\n",
    "\n",
    "# Sort and print the top 10 words along with their TF-IDF values\n",
    "sorted_tf_idf = sorted(tf_idf.items(), key=operator.itemgetter(1), reverse=True)\n",
    "for word, value in sorted_tf_idf[:10]:\n",
    "    print(f'{word}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use pos_tag() to tag each token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fulton', 'NN'),\n",
       " ('county', 'NN'),\n",
       " ('grand', 'JJ'),\n",
       " ('jury', 'NN'),\n",
       " ('said', 'VBD'),\n",
       " ('friday', 'JJ'),\n",
       " ('investig', 'JJ'),\n",
       " ('rec', 'NN'),\n",
       " ('prim', 'NN'),\n",
       " ('elect', 'VBP'),\n",
       " ('produc', 'NN'),\n",
       " ('evid', 'NN'),\n",
       " ('irregul', 'NN'),\n",
       " ('took', 'VBD'),\n",
       " ('plac', 'JJ'),\n",
       " ('jury', 'NN'),\n",
       " ('said', 'VBD'),\n",
       " ('pres', 'NNS'),\n",
       " ('city', 'NN'),\n",
       " ('execut', 'VBP')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use NLTK to tag \n",
    "tags = nltk.pos_tag(tokens_clean)\n",
    "tags[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print to the screen the 10 most common trigrams of word-tag pairs. Show their frequencies as well. Use nltk.trigrams(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('world', 'NN'), ('war', 'NN'), ('2', 'CD')): 35\n",
      "(('new', 'JJ'), ('york', 'NN'), ('city', 'NN')): 27\n",
      "(('new', 'JJ'), ('york', 'NN'), ('tim', 'NN')): 22\n",
      "(('index', 'NN'), ('word', 'NN'), ('electron', 'NN')): 21\n",
      "(('govern', 'JJ'), ('unit', 'NN'), ('stat', 'NN')): 18\n",
      "(('word', 'NN'), ('electron', 'NN'), ('switch', 'NN')): 18\n",
      "(('unit', 'NN'), ('stat', 'NN'), ('americ', 'JJ')): 16\n",
      "(('new', 'JJ'), ('york', 'NN'), ('cent', 'NN')): 15\n",
      "(('af', 'NN'), ('af', 'NN'), ('af', 'NN')): 15\n",
      "(('world', 'NN'), ('war', 'NN'), ('1', 'CD')): 14\n"
     ]
    }
   ],
   "source": [
    "# Generate trigrams of word-tag pairs\n",
    "trigrams_tags = list(nltk.trigrams((tags)))\n",
    "\n",
    "# Calculate the frequency distribution of the trigrams\n",
    "trigrams_freq = Counter(trigrams_tags)\n",
    "\n",
    "# Print the 10 most common trigrams along with their frequencies\n",
    "top_10_trigrams = trigrams_freq.most_common(10)\n",
    "for trigram, freq in top_10_trigrams:\n",
    "    print(f'{trigram}: {freq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
